{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATCHING KEYPOINTS DESCRIPTORS\n",
    "\n",
    " In last chapter we learn to extract keypoints using sift,surf,brief,fast and orb because we can use them for image matching.\n",
    " \n",
    " *** OUR GOAL IS TO FIND THE MATCHING POINTS BETWEEN THESE TWO IMAGES***\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKEND OF MATCHING\n",
    "\n",
    "WE use orb detector to extract the keypoints. once we extracted the keypoints,we used the BRUTE FORCE MATCHER to match the descriptors.\n",
    "\n",
    "*** BRUTE FORCE MATCHING IS PRETTY STRAIGHTFORWARD! FOR EVERY DESCRIPTOR IN THE FIRST IMAGE,WE MATCH IT WITH EVERY DESCRIPTOR IN SECOND IMAGE AND TAKE THE CLOSEST ONE.TO COMPUTE THE CLOSEST DESCRIPTOR,WE USE THE HAMMERING DISTANCE AS THE METRIC*** \n",
    "\n",
    " bf=cv2.BFMatcher(cv2.NORM_HAMMING,crossCheck=True)\n",
    " \n",
    " You can read more about the Hamming distance at https://en.wikipedia.org/\n",
    "wiki/Hamming_distance.\n",
    "\n",
    "*** The second argument in the preceding line is a Boolean\n",
    "variable. If this is true, then the matcher returns only those keypoints that are closest\n",
    "to each other in both directions. This means that if we get (i, j) as a match, then we\n",
    "can be sure that the i-th descriptor in the first image has the j-th descriptor in the\n",
    "second image as its closest match and vice versa. This increases the consistency\n",
    "and robustness of descriptor matching.***\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let code\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def draw_matches(img1,keypoints1,img2,keypoints2,matches):\n",
    "    \n",
    "    rows1,cols1=img1.shape[:2]\n",
    "    rows2,cols2=img2.shape[:2]\n",
    "    \n",
    "    #create a new output image that concatenate the two images togethor\n",
    "    \n",
    "    output_img=np.zeros((max([rows1,rows2]),cols1+cols2,3),dtype=np.uint8)\n",
    "    \n",
    "    output_img[:rows1,:cols1,:]=np.dstack([img1,img1,img1])\n",
    "    \n",
    "    output_img[:rows2,cols1:cols1+cols2,:]=np.dstack([img2,img2,img2])\n",
    "    \n",
    "    # draw  connecting lines between matching keypoints\n",
    "    \n",
    "    for match in matches:\n",
    "        \n",
    "        \n",
    "         #get the matching keypoints for each of the images\n",
    "        img1_idx=match.queryIdx\n",
    "        img2_idx=match.trainIdx\n",
    "        \n",
    "        (x1,y1)=keypoints1[img1_idx].pt\n",
    "        \n",
    "        (x2,y2)=keypoints2[img2_idx].pt\n",
    "        \n",
    "        #draw a small circle at both co-ordinates and then draw a line\n",
    "        \n",
    "        radius=4\n",
    "        color=(0,255,0)\n",
    "        thickness=4\n",
    "        cv2.circle(output_img,(int(x1),int(y1)),radius,color,thickness)\n",
    "        cv2.circle(output_img,(int(x2),int(y2)),radius,color,thickness)\n",
    "        \n",
    "        cv2.line(output_img,(int(x1),int(y1)),(int(x2)+cols1,int(y2)),color,thickness)\n",
    "        \n",
    "    return output_img\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    img1=cv2.imread('input2.jpg',0) # query image\n",
    "    \n",
    "    \n",
    "    img2=cv2.imread('input1.jpg',0) # train image\n",
    "    \n",
    "    \n",
    "    # initialize orb detector\n",
    "    \n",
    "    orb=cv2.ORB()\n",
    "    \n",
    "    #Extract keypoints and descriptors\n",
    "    \n",
    "    keypoints1,descriptors1=orb.detectAndCompute(img1,None)\n",
    "    keypoints2,descriptors2=orb.detectAndCompute(img2,None)\n",
    "    \n",
    "    \n",
    "    # create brute force matcher object\n",
    "    \n",
    "    bf=cv2.BFMatcher(cv2.NORM_HAMMING,crossCheck=True)\n",
    "    \n",
    "    # match descriptors\n",
    "    \n",
    "    matches=bf.match(descriptors1,descriptors2)\n",
    "    \n",
    "    #sort them in the order of their distance\n",
    "    \n",
    "    matches=sorted(matches,key=lambda x:x.distance)\n",
    "    \n",
    "    #draw first 'n' matches\n",
    "    \n",
    "    img2=draw_matches(img1,keypoints1,img2,keypoints2,matches[:30])\n",
    "    \n",
    "    cv2.imshow('matched keypoints',img3)\n",
    "    cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
