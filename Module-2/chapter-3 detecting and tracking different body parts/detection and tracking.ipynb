{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAAR CASCADES TO DETECT THINGS\n",
    "\n",
    "*** 2001 Paul Viola and Michael Jones***\n",
    "must read.\n",
    "The simple idea behind the paper was that building a robust single step classifier is a computationally  intensive process.There is always a trade off between speed and acurracy in machine learning.*** SO THE IDEA IS TO BUILD A SIMPLE CLASSIFIER AND CASCADE THEM TOGETHER TO FORM UNIFIED CLASSIFIER THAT'S IS ROBUST.***\n",
    "#2\n",
    "HAAR FEATURES ARE SIMPLE SUMMATION AND DIFFERENCE OF PATCHES ACROSS THE IMAGES.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTEGRAL IMAGES\n",
    "\n",
    "To find summation and difference of many different rectangular region within image is expensive process,to get rid of it we use concept of integral images.\n",
    "\n",
    "that is:-if *** AP indicate sum of all elements in rectangle from a rectangle with A and P as opposite corners.***\n",
    "\n",
    "now area of rectangle ABCD=AC-(AB+AD-AA)\n",
    "\n",
    "*** WE CARE FOR THIS FORMULA BECAUSE EXTRACTING HAAR FEATURE INCLUDES COMPUTING THE AREAS OF A LARGE NUMBER OF RECTANGLE IN THE IMAGE AT MULTIPLE SCALES.\n",
    "! The good thing about this approach\n",
    "is that we don't have to recalculate anything. All the values for the areas on the right\n",
    "hand side of this equation are already available. So we just use them to compute the\n",
    "area of any given rectangle and extract the features.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETECTING AND TRACKING FACES\n",
    "\n",
    "#1\n",
    "CascadeClassifier is used to load the xml file.*** The second argument\n",
    "in this function specifies the jump in the scaling factor. As in, if we don't find an\n",
    "image in the current scale, the next size to check will be, in our case, 1.3 times bigger\n",
    "than the current size. The last parameter is a threshold that specifies the number of\n",
    "adjacent rectangles needed to keep the current rectangle. It can be used to increase\n",
    "the robustness of the face detector.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "face_cascasde=cv2.CascadeClassifier('cascade\\haarcascade_frontalface_alt.xml')\n",
    "\n",
    "scale=0.5\n",
    "while True:\n",
    "    \n",
    "    ret,frame=cap.read()\n",
    "    \n",
    "    frame = cv2.resize(frame, None, fx=scale,\n",
    "                        fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    face_rect=face_cascasde.detectMultiScale(gray,1.3,5)\n",
    "    \n",
    "    for (x,y,w,h) in face_rect:\n",
    "        \n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),3)\n",
    "       \n",
    "        \n",
    "        \n",
    "    cv2.imshow('face dectector',frame)\n",
    "    \n",
    "    c=cv2.waitKey(1)\n",
    "    if c==27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "                                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLYING FILTER ON FACE\n",
    "\n",
    "e we know where the face is, we need to modify the coordinates a\n",
    "bit to make sure the mask fits properly. This manipulation process is subjective\n",
    "and depends on the mask in question. Different masks require different levels of\n",
    "adjustments to make it look more natural. We extract the region-of-interest from the\n",
    "input frame in the following line:\n",
    "frame_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "#2\n",
    "\n",
    "we resize the input mask to make sure it fits in this region of interest \n",
    "*** THE INPUT MASK HAS A WHITE BACKGROUND.SO ONLY THE FILTER MUST BE PRESENT BEFORE OVERLAPING AND MAKING THE REMAINING PORTION TRANSPARENT***\n",
    "\n",
    "#3\n",
    "we create a mask by thresholding the skull image. Since the\n",
    "background is white, we threshold the image so that any pixel with an intensity\n",
    "value greater than 180 becomes 0, and everything else becomes 255\n",
    "\n",
    "#4\n",
    "frame region-of-interest is concerned, we need to black out everything in this mask\n",
    "region. We can do that by simply using the inverse of the mask we just created. Once\n",
    "we have the masked versions of the skull image and the input region-of-interest, we\n",
    "just add them up to get the final image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6bd5a6a8cf35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mface_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hanniba.png'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# to read the png mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mh_m\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_m\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mface_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# COODE\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cascade=cv2.CascadeClassifier('cascade\\haarcascade_frontalface_alt.xml')\n",
    "\n",
    "face_mask=cv2.imread('hanniba.png') # to read the png mask\n",
    "\n",
    "h_m,w_m=face_mask.shape[:2]\n",
    "\n",
    "if face_cascade.empty():\n",
    "    raise IOError('unable to load the face cascade classifier')\n",
    "    \n",
    "    \n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "scale=0.5\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret,frame=cap.read()\n",
    "    \n",
    "    frame=cv2.resize(frame,fx=1.5,fy=1.5,interpolation=cv2.INTER_AREA)# TO RESIZE OUR SCRREN\n",
    "    \n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)# to convert into grayscale\n",
    "    \n",
    "    face=face_cascade.detectMultiScale(gray,1.3,5)\n",
    "    \n",
    "    \n",
    "    for (x,y,w,h) in face:\n",
    "        \n",
    "        if h>0 and w>0:\n",
    "            \n",
    "            # adjust the h and w depending on the sizes \n",
    "            \n",
    "            h,w=int(1.4*h),int(1*w)\n",
    "            \n",
    "            y-= 0.1*h\n",
    "            \n",
    "            # extract the reign of interest\n",
    "            \n",
    "            frame_roi=frame[y:y+h,x:x+w]\n",
    "            \n",
    "            \n",
    "            face_mask_small=cv2.resize(face_mask,(w,h),interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            #CONVERT color image to grayscale and threshold it\n",
    "            \n",
    "            gray_mask=cv2.cvtColor(face_mask_small,cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            ret,mask=cv2.threshold(gray_mask,180,255,cv2.THRESH_BINARY_INV)\n",
    "            \n",
    "            \n",
    "            # CREATE A INVERSE MASK\n",
    "            \n",
    "            mask_inv=cv2.bitwise_not(mask)\n",
    "            \n",
    "            # use the mask to extract the face mask region of interest\n",
    "            \n",
    "            masked_face=cv2.bitwise_and(face_mask_small,face_mask_small,mask=mask)\n",
    "            \n",
    "            # use the inverse mask to get the remaining part of the image\n",
    "            \n",
    "            masked_frame=cv2.bitwise_and(frame_roi,frame_roi,mask=mask_inv)\n",
    "            \n",
    "            # add the two images to get the final output\n",
    "            \n",
    "            frame[y:y+h,x:x+w]=cv2.add(masked_face,maske_frame)\n",
    "            \n",
    "            \n",
    "        cv2.imshow('face_filter',frame)\n",
    "        \n",
    "        c=cv2.waitKey(1)\n",
    "        if c==27:\n",
    "            break\n",
    "            \n",
    "            \n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETECTING EYES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "face_cascade=cv2.CascadeClassifier('cascade\\haarcascade_frontalface_alt.xml')\n",
    "eye_cascade=cv2.CascadeClassifier('cascade\\haarcascade_eye.xml')\n",
    "while True:\n",
    "    \n",
    "    ret,frame=cap.read()\n",
    "    \n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    face=face_cascade.detectMultiScale(gray,1.3,5)\n",
    "    \n",
    "    for(x,y,w,h) in face: # to get coordinate of face\n",
    "        \n",
    "        roi_gray=gray[y:y+h,x:x+w]\n",
    "        roi_color=frame[y:y+h,x:x+w]\n",
    "        \n",
    "        eye=eye_cascade.detectMultiScale(roi_gray)\n",
    "        \n",
    "        for (x,y,w,h) in eye:\n",
    "            \n",
    "            center=(int(x+w*0.5),int(y+h*0.5))\n",
    "            radius=int(0.3*(w+h))\n",
    "            color=(0,255,0)\n",
    "            thick=3\n",
    "            cv2.circle(roi_color,center,radius,color,thick)\n",
    "            \n",
    "    cv2.imshow('eye dectector',frame)\n",
    "    \n",
    "    c=cv2.waitKey(1)\n",
    "    if c==27:\n",
    "        break\n",
    "        \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETECTING THE PUPILS\n",
    "\n",
    "we can use shape analysis to detect the pupils.\n",
    "#2\n",
    "We invert the input image\n",
    "and then convert it into grayscale image as shown in the following line:\n",
    "gray = cv2.cvtColor(~img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#3\n",
    " Inverting\n",
    "the image is helpful in our case because the pupil is black in color, and black\n",
    "corresponds to a low pixel value. We then threshold the image to make sure that\n",
    "there are only black and white pixels. Now, we have to find out the boundaries of all\n",
    "the shapes\n",
    "#4\n",
    ". We can\n",
    "use the function boundingRect to obtain the coordinates of the bounding rectangle. \n",
    "#5\n",
    "We can use the function contourArea to compute the area of any contour in\n",
    "the image. So we can use these conditions and filter out the shapes. After we do that,\n",
    "we are left with two pupils in the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread('input.jpg')\n",
    "scaling_factor = 0.7\n",
    "img = cv2.resize(img, None, fx=scaling_factor,fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "cv2.imshow('Input', img)\n",
    "gray = cv2.cvtColor(~img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret, thresh_gray = cv2.threshold(gray, 220, 255,cv2.THRESH_BINARY)\n",
    "\n",
    "contours, hierarchy = cv2.findContours(thresh_gray,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "\n",
    "for contour in contours:\n",
    "    area = cv2.contourArea(contour)\n",
    "    rect = cv2.boundingRect(contour)\n",
    "    x, y, width, height = rect\n",
    "    radius = 0.25 * (width + height)\n",
    "    area_condition = (100 <= area <= 200)\n",
    "    symmetry_condition = (abs(1 - float(width)/float(height))\n",
    "                          <= 0.2)\n",
    "    fill_condition = (abs(1 - (area / (math.pi * math.pow(radius,\n",
    "                      2.0)))) <= 0.3)\n",
    "    if area_condition and symmetry_condition and fill_condition:\n",
    "        cv2.circle(img, (int(x + radius), int(y + radius)),int(1.3*radius), (0,255,0), -1)\n",
    "cv2.imshow('Pupil Detector', img)\n",
    "c = cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thank you\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
