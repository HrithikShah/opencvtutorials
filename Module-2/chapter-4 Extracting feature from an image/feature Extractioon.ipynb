{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHY keypoints?\n",
    "\n",
    "Image content anaylsis is the process of understanding the content of an image so that we can take some action based on that .\n",
    "when we build object recogniton system, we need to detect these \"INTERESTING\" regions to create a signature of the image and these interesting regions are characterized by *** KEYPOINTS***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT IS KEYPOINTS?\n",
    "\n",
    "as they are interesting points in image.for example, corners are interesting because there is sharp change in intensity in two different directions.\n",
    "#2\n",
    "***  It is important to understand\n",
    "that \"interesting\" doesn't necessarily refer to color or intensity values. It can be\n",
    "anything, as long as it is distinct.***\n",
    "\n",
    "we need to isolate the points that are unique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETECTING THE CORNERS\n",
    "\n",
    "POPULAR CORNER DETECTION TECHNIQUE:\n",
    "#1\n",
    "HARRIS CORNER DETECTOR\n",
    "\n",
    "#2 HOW IT WORKS.\n",
    "it construct 2x2 matrix based partial derivatives of grayscale image and then *** anaylze the eigenvalues\n",
    "#3\n",
    "***  This is actually an\n",
    "oversimplification of the actual algorithm, but it covers the gist. So, if you want to\n",
    "understand the underlying mathematical details, you can look into the original paper\n",
    "by Harris and Stephens at http://www.bmva.org/bmvc/1988/avc-88-023.pdf. A\n",
    "corner point is a point where both the eigenvalues would have large values***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code harris corner detector\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret,img=cap.read()\n",
    "\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    gray=np.float32(gray)\n",
    "\n",
    "    dst=cv2.cornerHarris(gray,4,5,0.04) # to detect only sharp corner\n",
    "\n",
    "#dst = cv2.cornerHarris(gray, 14, 5, 0.04) # to detect softcorners\n",
    "\n",
    "#result is dilated for making the corners\n",
    "    dst=cv2.dilate(dst,None)\n",
    "\n",
    "#threshold for an optimal value\n",
    "    img[dst>0.01*dst.max()]=[0,0,0]\n",
    "\n",
    "\n",
    "    cv2.imshow('harris corners',img)\n",
    "\n",
    "    c=cv2.waitKey(1)\n",
    "    \n",
    "    if c==27:\n",
    "        break\n",
    "        \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHI-TOMASI corner detector\n",
    "\n",
    "Around six years after the original paper by Harris and Stephens, Shi-Tomasi came\n",
    "up with a better corner detector. You can read the original paper at http://www.\n",
    "ai.mit.edu/courses/6.891/handouts/shi94good.pdf.\n",
    "\n",
    "*** IN THIS METHOD, WE FIND N STORNGEST CORNERS IN THE GIVEN IMAGE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET CODE SHI TOMASI CORNER DETECTOR.\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret,img=cap.read()\n",
    "\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "\n",
    "    dst=cv2.goodFeaturesToTrack(gray,7,0.05,25) # to detect only sharp corner\n",
    "\n",
    "#dst = cv2.cornerHarris(gray, 14, 5, 0.04) # to detect softcorners\n",
    "    dst=np.float32(dst)\n",
    "    \n",
    "    for  item in dst:\n",
    "        x,y=item[0]\n",
    "        cv2.circle(img,(x,y),5,255,-1)\n",
    "\n",
    "\n",
    "\n",
    "    cv2.imshow('harris corners',img)\n",
    "\n",
    "    c=cv2.waitKey(1)\n",
    "    \n",
    "    if c==27:\n",
    "        break\n",
    "        \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCALE INVARIANT FEATURE TRANSFORM(SIFT)\n",
    "\n",
    "Even though the corner are interesting but we want our image recognition to be invariant to *** SCALING,ROTATION,ILLUMINATION ETC***  it is possible that a corner in a image might not a corner in same enlarged image.it is one of drawback of using corner detection.\n",
    "\n",
    "#1 SIFT\n",
    "\n",
    "*** It is most popular algorithms in all computer vison.\n",
    " You can read\n",
    "David Lowe's original paper at http://www.cs.ubc.ca/~lowe/papers/ijcv04.\n",
    "pdf***\n",
    "\n",
    "\n",
    "it extract keypoints and build the corresponding feature description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.cv2' has no attribute 'SIFT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f01136223f32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minput_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'input.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgray_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msift\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIFT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mkeypoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msift\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m input_image = cv2.drawKeypoints(input_image, keypoints,\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2.cv2' has no attribute 'SIFT'"
     ]
    }
   ],
   "source": [
    "# lets code SIFT\n",
    "import cv2\n",
    "import numpy as np\n",
    "input_image = cv2.imread('input.jpg')\n",
    "gray_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n",
    "sift = cv2.SIFT()\n",
    "keypoints = sift.detect(gray_image, None)\n",
    "input_image = cv2.drawKeypoints(input_image, keypoints,\n",
    "flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "cv2.imshow('SIFT features', input_image)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPEED UP ROBUST FEATURES(SURF)\n",
    "SIFT is hard to implement on real time system.therefore we use SURF\n",
    "it is simple box filter to approximate the gaussian .it is easy to compute and its reasonably fast.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets code SURF\n",
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread('input.jpg')\n",
    "gray= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "surf = cv2.SURF()\n",
    "# This threshold controls the number of keypoints\n",
    "surf.hessianThreshold = 15000\n",
    "kp, des = surf.detectAndCompute(gray, None)\n",
    "img = cv2.drawKeypoints(img, kp, None, (0,255,0), 4)\n",
    "cv2.imshow('SURF features', img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAST FROM ACCELERATED SEGMENT TEST(FAST)\n",
    "\n",
    "it is just a keypoints detection.Once keypoints are detected,we need to use SIFT and SURF to compute the descriptor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let code \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "gray =cv2.imread('input.jpg',0)\n",
    "\n",
    "fast=cv2.FastFeatureDetector()\n",
    "\n",
    "# DETECT KEYPOINTS\n",
    "\n",
    "keypoints=fast.detect(gray,None)\n",
    "\n",
    "print(\"number of keypoints witn non max suppression=\",len(keypoints))\n",
    "\n",
    "\n",
    "# draw keypoints on top of the input image\n",
    "\n",
    "imgkey=cv2.drawKeypoints(gray,keypoints,color=(255,0,0))\n",
    "\n",
    "cv2.imshow('fast keypoints',imgkey)\n",
    "\n",
    "# disable nonmaxsuppression\n",
    "\n",
    "fast.setBool('nonmaxSuppression',False)\n",
    "\n",
    "#detect keypoints agains\n",
    "\n",
    "keypoints=fast.detect(gray,None)\n",
    "\n",
    "print(\"total keypoints without  nonmaxsuppression\",lwn(keypoints))\n",
    "\n",
    "#draw keypoints on top of the input image\n",
    "\n",
    "imgkeypoints_withoutnomax=cv2.drawKeypoints(gray,keypoints,color=(0,255,0))\n",
    "\n",
    "cv2.imshow('fast keypoints',imgkeypoints_withoutnomax)\n",
    "\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BINARY ROBUST INDEPENDENT ELEMENTARY FEATURES(BRIEF)\n",
    "\n",
    "It is method for extracting feature descriptors.it cannot detect the keypoints by itself ,so we need to use it in conjunction with a keypoints detector. the good thing about BRIEF is that its compact and fast.\n",
    "\n",
    "*** IT TAKED THE LIST OF INPUT KEYWORDS AND OUTPUTS AN UPDATED LIST.SO IF YOU RUN BRIEF ON THIS IMAGE,YOU WILL SEE SOMETHING LIKE THIS***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\features2d\\src\\matchers.cpp:1049: error: (-5:Bad argument) Unknown matcher name in function 'cv::DescriptorMatcher::create'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0a1831fdff21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#initate BRIEF Extractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mbrief\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDescriptorMatcher_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BRIEF\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# FIND THE KEYPOINTS WITH STAR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\features2d\\src\\matchers.cpp:1049: error: (-5:Bad argument) Unknown matcher name in function 'cv::DescriptorMatcher::create'\n"
     ]
    }
   ],
   "source": [
    "# LET CODE\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "gray=cv2.imread('input2.jpg',0)\n",
    "\n",
    "#initate fast detector\n",
    "fast=cv2.FastFeatureDetector()\n",
    "\n",
    "\n",
    "#initate BRIEF Extractor\n",
    "\n",
    "brief=cv2.DescriptorMatcher_create(\"BRIEF\")\n",
    "\n",
    "# FIND THE KEYPOINTS WITH STAR\n",
    "\n",
    "keypoints=fast.detect(gray,None)\n",
    "\n",
    "#compute the descriptor with BRIEF\n",
    "\n",
    "keypoints,descriptors=brief.compute(gray,keypoints)\n",
    "\n",
    "gray_keypoints=cv2.drawKeypoints(gray,keypoints,color=(0,255,0))\n",
    "\n",
    "cv2.imshow('BRIEF keypoints',gray_keypoints)\n",
    "cv2.waitkey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORIENTED FAST AND ROTATED BRIEF(ORB)\n",
    "\n",
    "it is alogrithm out form OpenCV labs. it,s fast,robust and opensource both sift and surf algorithms are patented and you can't use them for commercial purpose.*** this is why ORB is good in many ways\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let code\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "input_image=cv2.imread('input.jpg')\n",
    "\n",
    "gray=cv2.cvtColor(input_image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# INTIATE ORB object\n",
    "\n",
    "orb=cv2.ORB()\n",
    "\n",
    "# FIND THE KEYPOINTS WITH ORB\n",
    "\n",
    "keypoints=orb.detect(gray,None)\n",
    "\n",
    "# compute the description with ORB\n",
    "\n",
    "keypoints,descriptors=orb.compute(gray,keypoints)\n",
    "\n",
    "\n",
    "# draw only the location of the keypoints without size or orientation\n",
    "\n",
    "\n",
    "final_keypoints=cv2.drawKeypoints(input_image,keypoints,color=(0,255,0),flags=0)\n",
    "\n",
    "cv2.imshow('orbkeypoints',input_image)\n",
    "cv2.imshow('orbkeypoints',final_keypoints)\n",
    "\n",
    "cv2.waitKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
